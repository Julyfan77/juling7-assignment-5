{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=5, distance_metric='manhattan'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y.reset_index(drop=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        #print(\"Predicting...\")\n",
    "        probabilities = []\n",
    "    \n",
    "    # 遍历每个输入样本\n",
    "        for x in X:\n",
    "            # 计算当前样本 x 与所有训练样本的距离\n",
    "            distances = [self.compute_distance(x, x_train) for x_train in self.X_train]\n",
    "            # 获取最近的 k 个邻居的索引\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            # 根据最近邻的标签计算类别 1 的概率\n",
    "            prob = np.mean(self.y_train[k_indices])  # 标签 1 的概率\n",
    "            # 将概率存入列表\n",
    "            probabilities.append(prob)\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "            \n",
    "\n",
    "    def compute_distance(self, X1, X2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((X1 - X2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(X1 - X2))\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            # 防止除以零的情况，加入一个微小的偏移量 1e-10\n",
    "            dot_product = np.dot(X1, X2)\n",
    "            norm_X1 = np.linalg.norm(X1)\n",
    "            norm_X2 = np.linalg.norm(X2)\n",
    "            cosine_similarity = dot_product / (norm_X1 * norm_X2 + 1e-10)\n",
    "            return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def select_features(data, target='Exited', missing_threshold=0.2, unique_ratio_threshold=0.95, corr_threshold=0.05):\n",
    "    \"\"\"\n",
    "    根据缺失值比例、唯一值比例和相关性筛选合适的特征列。\n",
    "    \n",
    "    参数：\n",
    "    - data: DataFrame，包含特征和目标变量\n",
    "    - target: str，目标变量名称（默认 'Exited'）\n",
    "    - missing_threshold: float，缺失值比例的阈值（默认 0.5）\n",
    "    - unique_ratio_threshold: float，单一值比例的阈值（默认 0.95）\n",
    "    - corr_threshold: float，与目标变量的绝对相关性阈值（默认 0.1）\n",
    "    \n",
    "    返回：\n",
    "    - selected_features: List[str]，筛选出的合适特征列\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 删除缺失值比例超过阈值的列\n",
    "    missing_ratio = data.isnull().mean()\n",
    "    valid_columns = missing_ratio[missing_ratio < missing_threshold].index\n",
    "    data = data[valid_columns]\n",
    "\n",
    "    # 2. 删除单一值比例过高的列\n",
    "    unique_ratio = data.nunique() / len(data)\n",
    "    valid_columns = unique_ratio[unique_ratio < unique_ratio_threshold].index\n",
    "    data = data[valid_columns]\n",
    "\n",
    "    # 3. 计算与目标变量的相关性（仅数值特征）\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    correlations = numeric_data.corr()[target].abs()  # 绝对相关性\n",
    "    selected_columns = correlations[correlations > corr_threshold].index\n",
    "\n",
    "    # 4. 返回筛选出的列（不包含目标变量）\n",
    "    selected_features = list(selected_columns.drop(target, errors='ignore'))\n",
    "\n",
    "    print(f\"Selected Features: {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def remove_outliers(X, y, features, threshold=2.5):\n",
    "    \"\"\"根据 Z-score 去除离群点，并返回过滤后的 X 和 y。\"\"\"\n",
    "    z_scores = np.abs(zscore(X[features]))\n",
    "    mask = (z_scores < threshold).all(axis=1)  # 过滤掉有离群点的行\n",
    "\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "\n",
    "    # 重置索引，确保 X 和 y 对齐\n",
    "    X_filtered = X_filtered.reset_index(drop=True)\n",
    "    y_filtered = y_filtered.reset_index(drop=True)\n",
    "\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def preprocess_data(train_path, test_path):\n",
    "    # 读取训练和测试数据\n",
    "    train_data = pd.read_csv(train_path)[:5000]\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # 分离特征和目标变量\n",
    "    X_train = train_data.drop(['Exited'], axis=1)\n",
    "    y_train = train_data['Exited']\n",
    "\n",
    "    # 删除无关列\n",
    "    X_train = X_train.drop(['id','CustomerId','Surname'], axis=1)\n",
    "    X_test = test_data.drop(['id','CustomerId','Surname'], axis=1)\n",
    "\n",
    "    # 定义数值和分类特征\n",
    "    numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', \n",
    "                        'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "    categorical_cols = ['Geography', 'Gender']\n",
    "\n",
    "    # 去除离群点\n",
    "    #X_train, y_train = remove_outliers(X_train, y_train, numeric_features, threshold=3)\n",
    "\n",
    "    # # 建立数值和分类特征的预处理流水线\n",
    "    # numeric_transformer = Pipeline(steps=[\n",
    "    #     ('imputer', SimpleImputer(strategy='mean')),\n",
    "    #     ('scaler', StandardScaler())])\n",
    "\n",
    "    # categorical_transformer = Pipeline(steps=[\n",
    "    #     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    #     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    #numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    #categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a Pipeline object that applies the standard scaler to the numerical columns\n",
    "# and the one hot encoder to the categorical columns; then applies the knn classifier (k=5).\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "    # 对训练和测试数据进行预处理\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # # 使用 PCA 降维（选取8个主成分）\n",
    "    # pca = PCA(n_components=5)\n",
    "    # X_train = pca.fit_transform(X_train)\n",
    "    # X_test = pca.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "def cross_validate(X, y, knn, n_splits=2):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        # Use numpy indexing or after resetting index\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_val)\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def hyperparameter_tuning(X, y):\n",
    "    # the range of k is just an example, since it takes too long to choose. I make them run several times manually and record the score manually.\n",
    "    param_grid = {'k': [15], 'distance_metric': [ 'euclidean']}\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for k in param_grid['k']:\n",
    "        for metric in param_grid['distance_metric']:\n",
    "            knn = KNN(k=k, distance_metric=metric)\n",
    "            cv_scores = cross_validate(X, y, knn)\n",
    "            avg_score = np.mean(cv_scores)\n",
    "            print(k,cv_scores)\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = {'k': k, 'distance_metric': metric}\n",
    "    \n",
    "    return best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('cs-506-predicting-customer-churn-using-knn/train.csv', 'cs-506-predicting-customer-churn-using-knn/test.csv')\n",
    "\n",
    "\n",
    "# train_data = pd.read_csv('cs-506-predicting-customer-churn-using-knn/train.csv')\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with outlier :std=3 --- 0.78-0.79\n",
    "pca : 10 会降低\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 [np.float64(0.8965521341436599), np.float64(0.9088948353348693)]\n",
      "Best parameters: {'k': 15, 'distance_metric': 'euclidean'}\n",
      "Best cross-validation score: 0.9027234847392646\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score = hyperparameter_tuning(X, y)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best cross-validation score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "# try serveral times: k==7，9，...:\n",
    "knn = KNN(k=15, distance_metric=best_params['distance_metric'])\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save predictions to CSV\n",
    "pd.DataFrame({'id': pd.read_csv('cs-506-predicting-customer-churn-using-knn/test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs506",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
